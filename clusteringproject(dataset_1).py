# -*- coding: utf-8 -*-
"""ClusteringProject(Dataset-1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3aHnBKzXbK6kc1UzHizejzX_NlY5ftd
"""

from google.colab import drive
drive.mount("/content/gdrive/")

DATAPATH=r"/content/gdrive/My Drive/DM_datasets"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics

from sklearn.cluster import KMeans
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import Birch
from sklearn.cluster import DBSCAN
from sklearn.cluster import MiniBatchKMeans
from sklearn.cluster import MeanShift
from sklearn.cluster import OPTICS
from sklearn.cluster import SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots 
colors = ['#DB1C18','#DBDB3B','#51A2DB']
sns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})

df=pd.read_csv(f'{DATAPATH}/Country-data.csv')

df.head()

from sklearn.preprocessing import StandardScaler
df_scaled = StandardScaler().fit_transform(df.drop(['country'], axis=1))

from sklearn.decomposition import PCA
decom = PCA(svd_solver='auto')
decom.fit(df_scaled)

cum_exp_ratio = np.cumsum(np.round(decom.explained_variance_ratio_,2))
print(cum_exp_ratio)
fig=plt.figure(figsize=(10,8))
ax=sns.lineplot(y=cum_exp_ratio, x=np.arange(0,len(cum_exp_ratio)))
ax=sns.scatterplot(y=cum_exp_ratio, x=np.arange(0,len(cum_exp_ratio)))
ax.set_xlabel('No of components')
ax.set_ylabel('explaned variance ratio')

df['country'].count()

fig, ax = plt.subplots(nrows=3,ncols=3, figsize=(15,8), constrained_layout=True)
plt.suptitle("Univariated Data Analyis")
ax=ax.flatten()
int_cols= df.select_dtypes(exclude='object').columns
for x, i in enumerate(int_cols):
    sns.histplot(df[i], ax=ax[x], kde=True, color=colors[2])

fig, ax = plt.subplots(nrows=3,ncols=3, figsize=(15,8), constrained_layout=True)
plt.suptitle("Univariated Data Analyis")
ax=ax.flatten()
int_cols= df.select_dtypes(exclude='object').columns
for x, i in enumerate(int_cols):
    sns.boxplot(x=df[i], ax=ax[x], color=colors[2])

px.scatter(data_frame=df, x='exports', y='imports',size='gdpp', text='country', color='gdpp', title='Countries by Export & Import and corresponding GDP')

import scipy.cluster.hierarchy as sch
fig=plt.figure(figsize=(15,8))
dendrogram = sch.dendrogram(sch.linkage(df_scaled, method = 'ward'))
plt.suptitle('Hierarchial clustering - Dendrogram')
plt.xlabel('Countries')
plt.ylabel('Euclidean Distances')
plt.show()

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualize = KElbowVisualizer(model, k=(1,10))
visualize.fit(df_scaled)
visualize.poof()

output = pd.DataFrame(index=['K-Means','Mean-Shift','DBSCAN','Birch','Optics'],
                      columns=['SC','CH','DB'])

"""**KMEANS**"""

model = KMeans(n_clusters=3, random_state=1)
model.fit(df_scaled)
labels = model.labels_
df['KMean_labels']=model.labels_
fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))
sns.scatterplot(data=df, x='exports', y='income', hue='KMean_labels', ax=ax[0])
sns.scatterplot(data=df, x='exports', y='gdpp', hue='KMean_labels', ax=ax[1])
sns.scatterplot(data=df, x='child_mort', y='health', hue='KMean_labels', ax=ax[2])

output.loc['K-Means','SC'] = metrics.silhouette_score(df_scaled, labels, metric='euclidean')
output.loc['K-Means','CH'] = metrics.calinski_harabaz_score(df_scaled, labels)
output.loc['K-Means','DB'] = metrics.davies_bouldin_score(df_scaled, labels)

output

df.groupby(['KMean_labels','country']).mean()

from sklearn.metrics import silhouette_score
silhouette_score(df_scaled,labels=model.labels_)

"""**BIRCH**"""

from numpy import unique
model_br = Birch(threshold=0.01, n_clusters=4)
model_br.fit(df_scaled)
#
yhat_br = model_br.predict(df_scaled)
clusters_br = unique(yhat_br)
print("Clusters of Birch",clusters_br)
labels = model_br.labels_
df['Birch_labels']=model_br.labels_

output.loc['Birch','SC'] = metrics.silhouette_score(df_scaled, labels, metric='euclidean')
output.loc['Birch','CH'] = metrics.calinski_harabaz_score(df_scaled, labels)
output.loc['Birch','DB'] = metrics.davies_bouldin_score(df_scaled, labels)

score_br = metrics.silhouette_score(df_scaled,labels_br)

print("Score of Birch = ", score_br)

fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))
sns.scatterplot(data=df, x='exports', y='income', hue='Birch_labels', ax=ax[0])
sns.scatterplot(data=df, x='exports', y='gdpp', hue='Birch_labels', ax=ax[1])
sns.scatterplot(data=df, x='child_mort', y='health', hue='Birch_labels', ax=ax[2])

"""**OPTICS**"""

model_op = OPTICS(eps=0.8, min_samples=10)

yhat_op = model_op.fit_predict(df_scaled)
clusters_op = unique(yhat_op)
print("Clusters of Mean Shift.",clusters_op)
labels = model_op.labels_
df['OP_labels']=model_op.labels_

output.loc['Optics','SC'] = metrics.silhouette_score(df_scaled, labels, metric='euclidean')
output.loc['Optics','CH'] = metrics.calinski_harabaz_score(df_scaled, labels)
output.loc['Optics','DB'] = metrics.davies_bouldin_score(df_scaled, labels)

fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))
sns.scatterplot(data=df, x='exports', y='income', hue='OP_labels', ax=ax[0])
sns.scatterplot(data=df, x='exports', y='gdpp', hue='OP_labels', ax=ax[1])
sns.scatterplot(data=df, x='child_mort', y='health', hue='OP_labels', ax=ax[2])

score_op = metrics.silhouette_score(df_scaled,labels_op)

print("Score of Mean Shift = ", score_op)

"""**MEAN SHIFT**"""

clust_model = MeanShift(bandwidth=0.5)
clust_model.fit(df_scaled)
# Evaluating model's performance
print("Clusters of Mean Shift.",clust_model)
labels = clust_model.labels_
df['MS_labels']=clust_model.labels_

output.loc['Mean-Shift','SC'] = metrics.silhouette_score(df_scaled, labels, metric='euclidean')
output.loc['Mean-Shift','CH'] = metrics.calinski_harabaz_score(df_scaled, labels)
output.loc['Mean-Shift','DB'] = metrics.davies_bouldin_score(df_scaled, labels)

fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))
sns.scatterplot(data=df, x='exports', y='income', hue='MS_labels', ax=ax[0])
sns.scatterplot(data=df, x='exports', y='gdpp', hue='MS_labels', ax=ax[1])
sns.scatterplot(data=df, x='child_mort', y='health', hue='MS_labels', ax=ax[2])

"""**DBSCAN**"""

clust_model = DBSCAN(min_samples=6, eps=1)
clust_model.fit(df_scaled)
# Evaluating model's performance
labels = clust_model.labels_
df['DB_labels']=clust_model.labels_

output.loc['DBSCAN','SC'] = metrics.silhouette_score(df_scaled, labels, metric='euclidean')
output.loc['DBSCAN','CH'] = metrics.calinski_harabaz_score(df_scaled, labels)
output.loc['DBSCAN','DB'] = metrics.davies_bouldin_score(df_scaled, labels)

fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))
sns.scatterplot(data=df, x='exports', y='income', hue='DB_labels', ax=ax[0])
sns.scatterplot(data=df, x='exports', y='gdpp', hue='DB_labels', ax=ax[1])
sns.scatterplot(data=df, x='child_mort', y='health', hue='DB_labels', ax=ax[2])

"""# **Result**


"""

output